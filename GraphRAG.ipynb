{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105a2123",
   "metadata": {},
   "source": [
    "# Text2Cypher, Graph Retrieval Arguments Generation(RAG), Graph RAG and Graph+Vector RAG\n",
    "\n",
    "To enable Cognitive intelligence App towards private data, RAG + LLM and Knowledge Graph are state-of-the-art approaches.\n",
    "\n",
    "In this demo, we will explain know-how of four types of approaches and compare the trade-off and performance among them.\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "\n",
    "## Background, RAG\n",
    "\n",
    "Below digrams are showing how RAG works:\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In VectorDB based RAG, we create embeddings of each node(chunk), and find TopK related ones towards a given question during the query. In the above diagram, nodes `3` and `96` were fetched as the TopK related nodes, used to help answer the user query. \n",
    "\n",
    "## Background Graph RAG\n",
    "\n",
    "In Graph RAG, we will extract relationships between entities, representing concise facts from each node. It would look something like this:\n",
    "\n",
    "```\n",
    "Node Split and Embedding\n",
    "\n",
    "┌────┬────┬────┬────┐\n",
    "│ 1  │ 2  │ 3  │ 4  │\n",
    "├────┴────┴────┴────┤\n",
    "│  Docs/Knowledge   │\n",
    "│        ...        │\n",
    "├────┬────┬────┬────┤\n",
    "│ 95 │ 96 │    │    │\n",
    "└────┴────┴────┴────┘\n",
    "```\n",
    "\n",
    "Then, if we zoomed in of it:\n",
    "\n",
    "```\n",
    "       Node Split and Embedding, with Knowledge Graph being extracted\n",
    "\n",
    "┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
    "│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │\n",
    "│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │\n",
    "│ `▲'       `─'    │  `─'       `─'   │            `─'   │  `─'       `─'   │\n",
    "│  │     1         │        2         │        3    │    │        4         │\n",
    "│ .─.              │                  │            .▼.   │                  │\n",
    "│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │\n",
    "│ `◀────┐          │                  │            `─'  ││                  │\n",
    "├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤\n",
    "│       │                      Docs/Knowledge           │                   │\n",
    "│       │                            ...                │                   │\n",
    "│       │                                               │                   │\n",
    "├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤\n",
    "│  .─.  └──────.   │  .─.             │                 ││  .─.             │\n",
    "│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │\n",
    "│  `─'       `─'   │  `─'             │                  │  `─'             │\n",
    "│        95   │    │   │    96        │                  │   │    98        │\n",
    "│            .▼.   │  .▼.             │                  │   ▼              │\n",
    "│           ( c )  │ ( d )            │                  │  .─.             │\n",
    "│            `─'   │  `─'             │                  │ ( x )            │\n",
    "└──────────────────┴──────────────────┴──────────────────┴──`─'─────────────┘\n",
    "```\n",
    "\n",
    "Where, knowledge, the more granular spliting and information with higher density, optionally multi-hop of `x -> y`, `i -> j -> z -> x` etc... across many more nodes(chunks) than K(in TopK search) could be inlucded in Retrievers. And we believe there are cases that this additional work matters.\n",
    "\n",
    "But how/how well exactly does it work? Let's see in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75004d1",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f82010",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f42040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fdd991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223f9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468c6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1].append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5de50da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-azure-openai\n",
      "  Downloading llama_index_llms_azure_openai-0.1.5-py3-none-any.whl.metadata (786 bytes)\n",
      "Collecting azure-identity<2.0.0,>=1.15.0 (from llama-index-llms-azure-openai)\n",
      "  Downloading azure_identity-1.15.0-py3-none-any.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-llms-azure-openai) (0.27.0)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-llms-azure-openai)\n",
      "  Downloading llama_index_core-0.10.15-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.1 (from llama-index-llms-azure-openai)\n",
      "  Downloading llama_index_llms_openai-0.1.7-py3-none-any.whl.metadata (557 bytes)\n",
      "Collecting azure-core<2.0.0,>=1.23.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading azure_core-1.30.1-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting msal<2.0.0,>=1.24.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading msal-1.27.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading msal_extensions-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (6.0.1)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading SQLAlchemy-2.0.28-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.9.3)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2023.10.0)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl.metadata (762 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.1)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.12.0)\n",
      "Requirement already satisfied: pandas in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.2.1)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (4.9.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-llms-azure-openai) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-llms-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-llms-azure-openai) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-llms-azure-openai) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-llms-azure-openai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (4.0.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.6.2)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: packaging in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (23.2)\n",
      "Collecting portalocker<3,>=1.0 (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: click in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from anyio->httpx->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.2.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2024.1)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.16.3)\n",
      "Downloading llama_index_llms_azure_openai-0.1.5-py3-none-any.whl (4.5 kB)\n",
      "Downloading azure_identity-1.15.0-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_core-0.10.15-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.7-py3-none-any.whl (9.3 kB)\n",
      "Downloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msal-1.27.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msal_extensions-1.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.28-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.4/443.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.3/614.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dirtyjson, wrapt, tenacity, PyJWT, pycparser, portalocker, pillow, nltk, mypy-extensions, marshmallow, greenlet, typing-inspect, SQLAlchemy, deprecated, cffi, azure-core, llamaindex-py-client, dataclasses-json, cryptography, llama-index-core, msal, llama-index-llms-openai, msal-extensions, azure-identity, llama-index-llms-azure-openai\n",
      "Successfully installed PyJWT-2.8.0 SQLAlchemy-2.0.28 azure-core-1.30.1 azure-identity-1.15.0 cffi-1.16.0 cryptography-42.0.5 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 greenlet-3.0.3 llama-index-core-0.10.15 llama-index-llms-azure-openai-0.1.5 llama-index-llms-openai-0.1.7 llamaindex-py-client-0.1.13 marshmallow-3.21.1 msal-1.27.0 msal-extensions-1.1.0 mypy-extensions-1.0.0 nltk-3.8.1 pillow-10.2.0 portalocker-2.8.2 pycparser-2.21 tenacity-8.2.3 typing-inspect-0.9.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-graph-stores-nebula\n",
      "  Downloading llama_index_graph_stores_nebula-0.1.2-py3-none-any.whl.metadata (707 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-graph-stores-nebula) (0.10.15)\n",
      "Collecting nebula3-python<4.0.0,>=3.4.0 (from llama-index-graph-stores-nebula)\n",
      "  Downloading nebula3_python-3.5.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.12.0)\n",
      "Requirement already satisfied: pandas in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.9.0)\n",
      "Collecting future>=0.18.0 (from nebula3-python<4.0.0,>=3.4.0->llama-index-graph-stores-nebula)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httplib2>=0.20.0 (from nebula3-python<4.0.0,>=3.4.0->llama-index-graph-stores-nebula)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pytz>=2021.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nebula3-python<4.0.0,>=3.4.0->llama-index-graph-stores-nebula) (2024.1)\n",
      "Requirement already satisfied: six>=1.16.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nebula3-python<4.0.0,>=3.4.0->llama-index-graph-stores-nebula) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (4.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.16.0)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2>=0.20.0->nebula3-python<4.0.0,>=3.4.0->llama-index-graph-stores-nebula)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.6.2)\n",
      "Requirement already satisfied: anyio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.14.0)\n",
      "Requirement already satisfied: click in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-graph-stores-nebula) (2.16.3)\n",
      "Downloading llama_index_graph_stores_nebula-0.1.2-py3-none-any.whl (8.0 kB)\n",
      "Downloading nebula3_python-3.5.0-py3-none-any.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.3/326.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, future, httplib2, nebula3-python, llama-index-graph-stores-nebula\n",
      "Successfully installed future-1.0.0 httplib2-0.22.0 llama-index-graph-stores-nebula-0.1.2 nebula3-python-3.5.0 pyparsing-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-llms-openai in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (0.1.7)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-llms-openai) (0.10.15)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.12.0)\n",
      "Requirement already satisfied: pandas in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.6.2)\n",
      "Requirement already satisfied: anyio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-embeddings-azure-openai\n",
      "  Downloading llama_index_embeddings_azure_openai-0.1.6-py3-none-any.whl.metadata (803 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-embeddings-azure-openai) (0.10.15)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.3 (from llama-index-embeddings-azure-openai)\n",
      "  Downloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl.metadata (654 bytes)\n",
      "Requirement already satisfied: llama-index-llms-azure-openai<0.2.0,>=0.1.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-embeddings-azure-openai) (0.1.5)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.12.0)\n",
      "Requirement already satisfied: pandas in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.9.0)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.15.0)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (0.1.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.0.3)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (42.0.5)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.27.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.1.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.6.2)\n",
      "Requirement already satisfied: anyio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (23.2)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.16.3)\n",
      "Requirement already satisfied: pycparser in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (2.21)\n",
      "Downloading llama_index_embeddings_azure_openai-0.1.6-py3-none-any.whl (3.0 kB)\n",
      "Downloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: llama-index-embeddings-openai, llama-index-embeddings-azure-openai\n",
      "Successfully installed llama-index-embeddings-azure-openai-0.1.6 llama-index-embeddings-openai-0.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-azure-openai\n",
    "%pip install llama-index-graph-stores-nebula\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0db7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.15-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl.metadata (695 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.1.7-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.15 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index) (0.10.15)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index) (0.1.6)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index) (0.1.7)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.1.6-py3-none-any.whl.metadata (977 bytes)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-vector-stores-chroma<0.2.0,>=0.1.1 (from llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading llama_index_vector_stores_chroma-0.1.5-py3-none-any.whl.metadata (795 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.15->llama-index) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (1.12.0)\n",
      "Requirement already satisfied: pandas in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.15->llama-index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading PyMuPDF-1.23.26-cp39-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting llama-parse<0.4.0,>=0.3.3 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading llama_parse-0.3.5-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.15->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.15->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.15->llama-index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.16.0)\n",
      "Collecting chromadb<0.5.0,>=0.4.22 (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting onnxruntime<2.0.0,>=1.17.0 (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading onnxruntime-1.17.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.15.2)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.15->llama-index) (2.6.2)\n",
      "Requirement already satisfied: anyio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.15->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.15->llama-index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.9.0)\n",
      "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.15->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.15->llama-index) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.15->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.15->llama-index) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.15->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.15->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.15->llama-index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.2.0)\n",
      "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading build-1.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading fastapi-0.110.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading uvicorn-0.27.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading pulsar_client-3.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading importlib_resources-6.1.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading grpcio-1.62.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading mmh3-4.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading orjson-3.9.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.15->llama-index) (23.2)\n",
      "Collecting coloredlogs (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.25.3)\n",
      "Requirement already satisfied: sympy in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.12)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.15->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.15->llama-index) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.15->llama-index) (1.16.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.20.3)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (7.0.1)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: filelock in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.13.1)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting importlib-metadata>=4.6 (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_proto-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (68.2.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading httptools-0.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading uvloop-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading watchfiles-0.21.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading websockets-12.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from importlib-resources->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.17.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/yuansui/miniconda3/envs/rog/lib/python3.9/site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading llama_index-0.10.15-py3-none-any.whl (5.6 kB)\n",
      "Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_cli-0.1.7-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.6-py3-none-any.whl (34 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading llama_index_vector_stores_chroma-0.1.5-py3-none-any.whl (4.7 kB)\n",
      "Downloading llama_parse-0.3.5-py3-none-any.whl (7.7 kB)\n",
      "Downloading PyMuPDF-1.23.26-cp39-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.17.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
      "Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pulsar_client-3.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.9/186.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Downloading uvloop-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.21.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.0/130.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=01558dc6e3310075893fbae63817bcbc7a71da08f8466ca21197e626e91a6e23\n",
      "  Stored in directory: /home/yuansui/.cache/pip/wheels/f7/02/64/d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, mmh3, flatbuffers, websockets, websocket-client, uvloop, uvicorn, typer, tomli, pypdf, PyMuPDFb, pyasn1, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, oauthlib, importlib-resources, importlib-metadata, humanfriendly, httptools, grpcio, googleapis-common-protos, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, starlette, rsa, requests-oauthlib, pyproject_hooks, pymupdf, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, bs4, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, google-auth, fastapi, build, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, llama-index-legacy, kubernetes, opentelemetry-instrumentation-fastapi, llama-parse, llama-index-readers-file, llama-index-indices-managed-llama-cloud, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-agent-openai, chromadb, llama-index-vector-stores-chroma, llama-index-program-openai, llama-index-question-gen-openai, llama-index-cli, llama-index\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 7.0.1\n",
      "    Uninstalling importlib-metadata-7.0.1:\n",
      "      Successfully uninstalled importlib-metadata-7.0.1\n",
      "Successfully installed PyMuPDFb-1.23.22 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.2 build-1.1.1 cachetools-5.3.3 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 fastapi-0.110.0 flatbuffers-23.5.26 google-auth-2.28.1 googleapis-common-protos-1.62.0 grpcio-1.62.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.2 kubernetes-29.0.0 llama-index-0.10.15 llama-index-agent-openai-0.1.5 llama-index-cli-0.1.7 llama-index-indices-managed-llama-cloud-0.1.3 llama-index-legacy-0.9.48 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.6 llama-index-readers-llama-parse-0.1.3 llama-index-vector-stores-chroma-0.1.5 llama-parse-0.3.5 mmh3-4.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.17.1 opentelemetry-api-1.23.0 opentelemetry-exporter-otlp-proto-common-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.23.0 opentelemetry-instrumentation-0.44b0 opentelemetry-instrumentation-asgi-0.44b0 opentelemetry-instrumentation-fastapi-0.44b0 opentelemetry-proto-1.23.0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 opentelemetry-util-http-0.44b0 orjson-3.9.15 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pymupdf-1.23.26 pypdf-4.1.0 pypika-0.48.9 pyproject_hooks-1.0.0 requests-oauthlib-1.3.1 rsa-4.9 starlette-0.36.3 tomli-2.0.1 typer-0.9.0 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websocket-client-1.7.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061a39e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# define LLM\n",
    "# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\n",
    "llm = OpenAI(temperature=0, model=\"text-davinci-002\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://<foo-bar>.openai.azure.com\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_llm,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c8552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17442e",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ca9fa",
   "metadata": {},
   "source": [
    "❗Access NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cf0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nebula3-python in /opt/homebrew/lib/python3.11/site-packages/nebula3_python-3.4.0-py3.11.egg (3.4.0)\n",
      "Requirement already satisfied: ipython-ngql in /opt/homebrew/lib/python3.11/site-packages/ipython_ngql-0.5-py3.11.egg (0.5)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages/httplib2-0.22.0-py3.11.egg (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in /opt/homebrew/lib/python3.11/site-packages/future-0.18.3-py3.11.egg (from nebula3-python) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (2022.7.1)\n",
      "Requirement already satisfied: Jinja2 in /opt/homebrew/lib/python3.11/site-packages/Jinja2-3.1.2-py3.11.egg (from ipython-ngql) (3.1.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from ipython-ngql) (1.5.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/homebrew/lib/python3.11/site-packages/pyparsing-3.1.0b1-py3.11.egg (from httplib2>=0.20.0->nebula3-python) (3.1.0b1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages/MarkupSafe-2.1.2-py3.11-macosx-13-arm64.egg (from Jinja2->ipython-ngql) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->ipython-ngql) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas->ipython-ngql) (1.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nebula3-python ipython-ngql\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\" # assumed we have NebulaGraph installed locally\n",
    "\n",
    "space_name = \"guardians\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5613a",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "\n",
    "In our demo, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7083",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Data\n",
    "\n",
    "We will download and preprecess data from:\n",
    "    https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_Vol._3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c376da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21c03b",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f3668",
   "metadata": {},
   "source": [
    "This call will take some time, it'll extract entities and relationships and store them into NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c05437d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3efdc4",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG\n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In Llama Index, this could be done with oneline of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c2aee",
   "metadata": {},
   "source": [
    "## 4. Persist and Load from disk Llama Indexes(Optional)\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once, afterwards, we could persist their in-memory context to enable their reuse from disk anytime.\n",
    "\n",
    "#### Persist\n",
    "\n",
    "```python\n",
    "# persist KG Index(Only MetaData will be persisted, KG is in NebulaGraph)\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "\n",
    "# persist Vector Index\n",
    "vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "\n",
    "```\n",
    "\n",
    "Then the files are created:\n",
    "\n",
    "```bash\n",
    "$ ls -l ./storage_*\n",
    "\n",
    "storage_graph:\n",
    "total 6384\n",
    "-rw-r--r--@ 1 weyl  staff    44008 Jul 14 11:06 docstore.json\n",
    "-rw-r--r--@ 1 weyl  staff  3219385 Jul 14 11:06 index_store.json\n",
    "-rw-r--r--@ 1 weyl  staff       51 Jul 14 11:06 vector_store.json\n",
    "\n",
    "storage_vector:\n",
    "total 712\n",
    "-rw-r--r--@ 1 weyl  staff   44008 Jul 14 11:06 docstore.json\n",
    "-rw-r--r--@ 1 weyl  staff      18 Jul 14 11:06 graph_store.json\n",
    "-rw-r--r--@ 1 weyl  staff    1003 Jul 14 11:06 index_store.json\n",
    "-rw-r--r--@ 1 weyl  staff  311028 Jul 14 11:06 vector_store.json\n",
    "```\n",
    "\n",
    "#### Restore\n",
    "\n",
    "So we could restore the index from disk like:\n",
    "\n",
    "```python\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb936",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5524b2e",
   "metadata": {},
   "source": [
    "### 5.1 text-to-NebulaGraphCypher\n",
    "\n",
    "Text-to-NebulaGraphCypher approach Translate task/question into a Graph Cypher Query, and answer based on its query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097b25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c590de2",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06682474",
   "metadata": {},
   "source": [
    "### 5.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37713b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93396d7",
   "metadata": {},
   "source": [
    "### 5.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0ee74",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9516c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe22b2b",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb2d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67a63f",
   "metadata": {},
   "source": [
    "And the query engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4976682",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541608be",
   "metadata": {},
   "source": [
    "## 6. Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384be7b",
   "metadata": {},
   "source": [
    "### 6.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6acaa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mGraph Store Query: MATCH (p:`entity`)-[:relationship]->(e:`entity`) WHERE p.`entity`.`name` == 'Peter Quill' RETURN e.`entity`.`name`;\n",
      "\u001b[0mINFO:llama_index.query_engine.knowledge_graph_query_engine:Graph Store Query: MATCH (p:`entity`)-[:relationship]->(e:`entity`) WHERE p.`entity`.`name` == 'Peter Quill' RETURN e.`entity`.`name`;\n",
      "Graph Store Query: MATCH (p:`entity`)-[:relationship]->(e:`entity`) WHERE p.`entity`.`name` == 'Peter Quill' RETURN e.`entity`.`name`;\n",
      "\u001b[33;1m\u001b[1;3mGraph Store Response: {'e.entity.name': ['Guardians of the Galaxy']}\n",
      "\u001b[0mINFO:llama_index.query_engine.knowledge_graph_query_engine:Graph Store Response: {'e.entity.name': ['Guardians of the Galaxy']}\n",
      "Graph Store Response: {'e.entity.name': ['Guardians of the Galaxy']}\n",
      "\u001b[32;1m\u001b[1;3mFinal Response: \n",
      "Peter Quill is a member of the superhero team known as the Guardians of the Galaxy.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is a member of the superhero team known as the Guardians of the Galaxy.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cypher Query:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```cypher\n",
       "MATCH (p:`entity`)-[:relationship]->(e:`entity`) \n",
       "  WHERE p.`entity`.`name` == 'Peter Quill' \n",
       "RETURN e.`entity`.`name`;\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"Tell me about Peter Quill?\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafef8",
   "metadata": {},
   "source": [
    "### 6.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dec5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about Peter Quill.\n",
      "> Starting query: Tell me about Peter Quill.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is the leader of the Guardians of the Galaxy, a superhero team from Marvel Comics. He was portrayed by Chris Pratt in the 2014 movie of the same name, and reprised his role in the 2017 sequel. He also wrote and directed the first movie. Quill is known for his foul language, often using the word \"fuck\".</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c800",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08fa71a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill, also known as Star-Lord, is the half-human, half-Celestial leader of the Guardians of the Galaxy. He was abducted from Earth as a child and raised by a group of alien thieves and smugglers, the Ravagers. In the film, Quill is in a \"state of depression\" following the appearance of a variant of his dead lover Gamora, who does not share the same affection for Quill as her older version had for him, which in turn affects his leadership of the Guardians.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f256d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Result | Graph | Vector |\n",
       "| --- | --- | --- |\n",
       "| Character | Leader of the Guardians of the Galaxy | Half-human, half-Celestial leader of the Guardians of the Galaxy |\n",
       "| Movie | Portrayed by Chris Pratt in the 2014 movie of the same name, and reprised his role in the 2017 sequel. He also wrote and directed the first movie. | In the film, Quill is in a \"state of depression\" following the appearance of a variant of his dead lover Gamora, who does not share the same affection for Quill as her older version had for him, which in turn affects his leadership of the Guardians. |\n",
       "| Personality | Known for his foul language, often using the word \"fuck\". | No mention |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the two QA result on \"Tell me about Peter Quill.\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e1b07",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc59b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about Peter Quill.\n",
      "> Starting query: Tell me about Peter Quill.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is the half-human, half-Celestial leader of the Guardians of the Galaxy, a group of alien thieves and smugglers. He was abducted from Earth as a child and raised by the Ravagers. Quill reprised his role from the 2014 film Guardians of the Galaxy, which he also directed and wrote. He is the sequel to the original Guardians of the Galaxy and is known to speak with a foul mouth.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f82da8",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697e4bc",
   "metadata": {},
   "source": [
    "### 7.1 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbae28",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed1e85e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Knowledge Fact | text2GraphQuery | Graph | Vector | Graph+Vector |\n",
       "| -------------- | --------------- | ----- | ------ | ----------- |\n",
       "| Member of Guardians of the Galaxy | Yes | Yes | No | Yes |\n",
       "| Leader of Guardians of the Galaxy | No | Yes | No | Yes |\n",
       "| Portrayed by Chris Pratt | No | Yes | No | Yes |\n",
       "| Wrote and directed first movie | No | Yes | No | Yes |\n",
       "| Foul language | No | Yes | No | Yes |\n",
       "| Half-human, half-Celestial | No | No | Yes | Yes |\n",
       "| Abducted from Earth as a child | No | No | Yes | Yes |\n",
       "| Raised by a group of alien thieves and smugglers | No | No | Yes | Yes |\n",
       "| In a \"state of depression\" | No | No | Yes | No |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377a880",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae85ed",
   "metadata": {},
   "source": [
    "### 7.2 Text2GraphQuery vs Graph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48a35d",
   "metadata": {},
   "source": [
    "In **Text2GraphQuery**, we leverage the LLM to compose a Graph Query that's trying to provide the answer in the `RETURN` fields. While, on the other hands, the **Graph RAG** find all related knowledges as the context.\n",
    "\n",
    "So when the answer by nature refers to small piece of information as answer, Text2GraphQuery could do the job in the best, and in other cases, the Graph RAG could be better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4510b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Feature | Text-to-Graph | Graph RAG |\n",
       "| --- | --- | --- |\n",
       "| Superhero Team | Guardians of the Galaxy | Guardians of the Galaxy |\n",
       "| Movie | N/A | 2014 and 2017 |\n",
       "| Role | N/A | Portrayed by Chris Pratt |\n",
       "| Writing/Directing | N/A | Wrote and Directed the first movie |\n",
       "| Character Trait | N/A | Foul language, often using the word \"fuck\" |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the two QA result on \"Tell me about Peter Quill.\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from text-to-Graph: {response_nl2kg}\n",
    "---\n",
    "Result from Graph RAG: {response_graph_rag}\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33632e85",
   "metadata": {},
   "source": [
    "We could visulize the knowledge context that were coverred during the two approach, we could see their difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a01d88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_text2cypher = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53e78497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})&lt;-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              path_0\n",
       "0  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "1  (\"Peter Quill\" :entity{name: \"Peter Quill\"})<-...\n",
       "2  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "3  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "4  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "5  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "MATCH path_0=(p:`entity`)-[*1..2]-()\n",
    "  WHERE p.`entity`.`name` == 'Peter Quill' \n",
    "RETURN path_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d05dbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67f8f597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Text2Cypher Traversed knowledge</th>\n",
       "        <th>Graph Rag Traversed knowledge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>\n",
       "<iframe\n",
       "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_nl2cypher.html\"\n",
       "    width=450\n",
       "    height=400>\n",
       "</iframe>\n",
       "</td>\n",
       "        <td>\n",
       "<iframe\n",
       "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_rag.html\"\n",
       "    width=450\n",
       "    height=400>\n",
       "</iframe>\n",
       "</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2Cypher = \"\"\"\n",
    "<iframe\n",
    "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_nl2cypher.html\"\n",
    "    width=450\n",
    "    height=400>\n",
    "</iframe>\n",
    "\"\"\"\n",
    "graphRAG = \"\"\"\n",
    "<iframe\n",
    "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_rag.html\"\n",
    "    width=450\n",
    "    height=400>\n",
    "</iframe>\n",
    "\"\"\"\n",
    "\n",
    "table = f\"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Text2Cypher Traversed knowledge</th>\n",
    "        <th>Graph Rag Traversed knowledge</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>{text2Cypher}</td>\n",
    "        <td>{graphRAG}</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be0b42",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea86302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paths: [[0, 2, 2, 2], [0, 2, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "def expand_path(path):\n",
    "    \"\"\"Generate all possible expansions for a given path.\"\"\"\n",
    "    # Placeholder: generate next steps from the last element of the path\n",
    "    return [path + [next_step] for next_step in range(3)]  # Example expansion\n",
    "\n",
    "def score_path(path):\n",
    "    \"\"\"Score a path. Higher scores are better.\"\"\"\n",
    "    # Placeholder scoring function\n",
    "    return sum(path)  # Example: score based on sum of elements\n",
    "\n",
    "def beam_search(initial_paths, k, max_length):\n",
    "    \"\"\"Perform a top-k beam search.\"\"\"\n",
    "    candidates = initial_paths\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Expand all candidates\n",
    "        all_expansions = []\n",
    "        for path in candidates:\n",
    "            all_expansions.extend(expand_path(path))\n",
    "        \n",
    "        # Score and select top-k\n",
    "        candidates = sorted(all_expansions, key=score_path, reverse=True)[:k]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# Initial setup\n",
    "initial_paths = [[0]]  # Starting with a path containing just a start node\n",
    "k = 2  # Beam width\n",
    "max_length = 3  # Maximum path length\n",
    "\n",
    "# Perform the search\n",
    "best_paths = beam_search(initial_paths, k, max_length)\n",
    "print(\"Best paths:\", best_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd25992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [1, 14, 70, 54, 73, 95, 82, 88, 87, 84, 21], Score: -38.92681906913212\n",
      "Sequence: [1, 14, 70, 54, 73, 95, 82, 88, 87, 84, 50], Score: -38.92904078656555\n",
      "Sequence: [1, 14, 70, 54, 73, 95, 82, 88, 87, 97, 14], Score: -38.9448954962864\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_k_beam_search(model_predict, start_sequence, k, beam_size, max_length):\n",
    "    \"\"\"\n",
    "    Performs top-k beam search.\n",
    "\n",
    "    Parameters:\n",
    "    - model_predict: a function to predict the next token probabilities.\n",
    "    - start_sequence: starting sequence of tokens.\n",
    "    - k: the number of top tokens to consider at each step.\n",
    "    - beam_size: the number of sequences to keep.\n",
    "    - max_length: the maximum length of the sequence to generate.\n",
    "\n",
    "    Returns:\n",
    "    - A list of top sequences and their scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with an empty beam\n",
    "    beam = [(start_sequence, 0.0)]  # Each element is a tuple (sequence, score)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        candidates = []\n",
    "        for seq, score in beam:\n",
    "            # Predict the next token probabilities\n",
    "            probs = model_predict(seq)\n",
    "            \n",
    "            # Select the top-k tokens\n",
    "            top_indices = np.argsort(probs)[-k:]\n",
    "            for i in top_indices:\n",
    "                next_seq = seq + [i]  # Append the top token to the sequence\n",
    "                # Log probabilities are used to prevent underflow\n",
    "                next_score = score + np.log(probs[i])\n",
    "                candidates.append((next_seq, next_score))\n",
    "\n",
    "        # Keep top beam_size sequences\n",
    "        ordered = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        beam = ordered[:beam_size]\n",
    "\n",
    "    return beam\n",
    "\n",
    "# Example usage:\n",
    "# Define a dummy model_predict function for demonstration\n",
    "def dummy_model_predict(sequence):\n",
    "    \"\"\"\n",
    "    A dummy prediction function that returns a random probability distribution over a hypothetical vocabulary.\n",
    "    The vocabulary size is assumed to be 100 for this example.\n",
    "    \"\"\"\n",
    "    vocab_size = 100\n",
    "    probs = np.random.rand(vocab_size)\n",
    "    probs /= probs.sum()  # Normalize to get probabilities\n",
    "    return probs\n",
    "\n",
    "# Starting sequence (assuming tokens are represented as integers)\n",
    "start_sequence = [1]\n",
    "\n",
    "# Parameters for the search\n",
    "k = 5\n",
    "beam_size = 3\n",
    "max_length = 10\n",
    "\n",
    "# Perform the search\n",
    "top_sequences = top_k_beam_search(dummy_model_predict, start_sequence, k, beam_size, max_length)\n",
    "\n",
    "# Print the results\n",
    "for seq, score in top_sequences:\n",
    "    print(f\"Sequence: {seq}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5850a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
